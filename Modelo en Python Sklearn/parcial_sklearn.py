# -*- coding: utf-8 -*-
"""Parcial_Sklearn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16bPjkq0HvbdXkAF4_jZ9_WXoBlbpgHGS

Estudiante:Diego Bermúdez. 

Docente:John Corredor, PhD.

Asignatura: HPC

Parcial final 

28-05-22

En el presente cuaderno se presentaran los diferentes pasos y requerimientos necesarios para la realizar el analisis exploratorio de datos en un dataSet.
Para el presente trabajo se manejara un dataSet llamado "povertyUSA_HPC.csv".
"""

# Se importan las bibliotecas necesarias

import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
import numpy as np
import statistics as st
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.compose import make_column_selector
from sklearn.model_selection import cross_validate
from sklearn.model_selection import RepeatedKFold

"""Dado que el data set se encontraba en un orden poco viable se realizó la adecuación del mismo, para su adecuado uso. El ordenamiento se realizó debido a que la variable dependiente o a estimar se encontraba en la primera columna. Para el correcto manejo de esta información fue necesario dejar a las variables independientes en las primeras 3 columnas del data set y que la ultima columna(4) fuese la variable dependiente.
La variable teenBrth fue descartada, debido a que nos brinda informacion poco precisa pues su rango de edad es difuso y nos afectaria los posibles resultados de la prueba.
Es cargado de este modo para obtener los "mismos" resultados que en el proyecto planteado en C++.
"""

# Se carga a un objeto dataFrame la informacion
poverty = pd.read_csv("/content/povertyUSA_HPC.csv")
# Se muestran los primeros 5 conjuntos de datos (variables independientes) - (variable dependiente)
poverty.head()

# Se muestra la informacion del dataFrame
# Tipos de datos nombre de los mismos y su cantidad
poverty.info()

# Se muestran los ultimos 5 conjuntos de datos (variables independientes) - (variable dependiente)
poverty.tail()

"""EDA del DataFrame povertyUSA_HPC

"""

# Se presenta un resumen estadistico del DataFrame
poverty.describe()

# Se muestra el numero de observaciones en el dataSet (cantidad de rows o conjuntos de datos)
poverty.shape[0]

# Se muestra el numero de columnas en el dataSet (cantidad de variables)
poverty.shape[1]

# Se muestran los nombres de las variables
poverty.columns

# Se muestra como se encuentran indexados los datos
poverty.index

# Se muestra el tipo de dato de cada variable
poverty.dtypes

# Se imprime la columna correspondiente a la variable dependiente "PovPct"
print(poverty['PovPct'])

# Se muestran los valores mas recurrente en los datos, referente a la variable a estimar
poverty['PovPct'].value_counts().head()

# Se muestra el valor medio en la variable "Porcentaje de Pobreza"
round(poverty['PovPct'].mean())

# Se quiere mostrar el promedio por columna de cada variable
promedio = np.zeros(shape=4)
promedio[0]=poverty['Brth15to17'].mean()
promedio[1]=poverty['Brth18to19'].mean()
promedio[2]=poverty['ViolCrime'].mean()
promedio[3]=poverty['PovPct'].mean()
print(" Se muestran los promedios para cada conjunto (columna-variable) de información ")
print(" Brth15to17  Brth18to19   ViolCrime   PovPct")
print(promedio)


# Ejemplo tomado de:
# https://www.delftstack.com/es/howto/python/get-average-of-list-python/
# https://www.delftstack.com/es/howto/numpy/empty-numpy-array/
# Nos muestra el promedio existente en cada columna

def normalizacion(datos):
    return datos.apply(lambda x: (x-x.mean())/ x.std(), axis=0)

datosNormalizados = normalizacion(poverty)
print(datosNormalizados)

# Ejemplo tomado de:
# https://www.delftstack.com/es/howto/python-pandas/pandas-normalize/
# Nos muestra la diferencia existente entre cada dato con el promedio de su columna correspondiente

desviacion = np.zeros( shape=4)
desviacion[0] = st.pstdev(poverty['Brth15to17'])
desviacion[1] = st.pstdev(poverty['Brth18to19'])
desviacion[2] = st.pstdev(poverty['ViolCrime'])
desviacion[3] = st.pstdev(poverty['PovPct'])

print(" Se muestra la desviación estandar para cada conjunto (columna-variable) de información ")
print(" Brth15to17  Brth18to19   ViolCrime   PovPct")
print(desviacion)

# Se presenta la matriz de correlación.
matCorr=poverty.corr()

plt.figure(figsize=(12,6))
sns.heatmap(matCorr,annot=True, cmap='Spectral')
plt.show()

# Se pinta la distribución de la variable dependiente
fig, axes = plt.subplots(1,1, figsize=(12,5))
plt.suptitle('Distribución de las variables en el DF')
plt.hist(poverty ['PovPct'])

"""Dado que existen 3 variables dependientes, es decir que inciden en el indice de pobreza, se mostraran graficas una a una por cada variable dependiente frente al indice de pobreza"""

# Se pinta la distribución de las variables. 
# Se pintaran dos variables (dependiente -- independiente)
fig, axes = plt.subplots(1,2, figsize=(12,5))
plt.suptitle('Distribución de las variables en el DF')
sns.histplot(poverty['Brth15to17'], ax=axes[0],kde=True)
sns.histplot(poverty['PovPct'], ax=axes[1],kde=True, color='r')
plt.show()

"""Se observa parcialmente que el comportamiento de la variable "Brth15to17" posee el comportamiento mas similar a el indice de pobreza, con los datos suministrados."""

# Se pinta la distribución de las variables. 
# Se pintaran dos variables (dependiente -- independiente)
fig, axes = plt.subplots(1,2, figsize=(12,5))
plt.suptitle('Distribución de las variables en el DF')
sns.histplot(poverty['Brth18to19'], ax=axes[0],kde=True)
sns.histplot(poverty['PovPct'], ax=axes[1],kde=True, color='r')
plt.show()

# Se pinta la distribución de las variables. 
# Se pintaran dos variables (dependiente -- independiente)
fig, axes = plt.subplots(1,2, figsize=(12,5))
plt.suptitle('Distribución de las variables en el DF')
sns.histplot(poverty['ViolCrime'], ax=axes[0],kde=True)
sns.histplot(poverty['PovPct'], ax=axes[1],kde=True, color='r')
plt.show()

correlaciones=sns

"""Dadas las graficas de distribución anteriores, se presentan con la distribución esperada, es decir distribución Gaussiana o Normal"""

# Se separan los datos en dos conjuntos: variables dependientes e independiente
# Variables independientes X
# Dado que para el caso en particular tenemos 3 variables independientes 
# sus datos son extraidos y cargados en una "unica" variable con la cual 
# contrastara sus datos la variable dependiente
X=poverty.drop(['PovPct'], axis=1)
# Variable dependiente
y=poverty['PovPct']

# Se crea un modelo de regresión lineal
modelo = linear_model.LinearRegression()
 
# Se entrena el modelo con los datos (X,Y)
modelo.fit(X, y)
# Ahora se puede obtener el coeficiente b_1
print('Coeficiente beta1: ', modelo.coef_[0])
 
# Podemos predecir usando el modelo
y_pred = modelo.predict(X)
 
# Por último, calculamos el error cuadrático medio y el estadístico R^2
print('Error cuadrático medio: %.2f' % mean_squared_error(y, y_pred))
print('Estadístico R_2: %.2f' % r2_score(y, y_pred))

# Se dividen los grupos en entrenamiento y prueba
X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2,shuffle=False)
# Se realiza el Modelo en RL en Sklearn: Se hace uso de la actualización
# actualización de Sklearn para los modelos de predicción
pipe = Pipeline([('scaler', StandardScaler()), ('LR', LinearRegression())])
# Se entrena el modelo para el conjunto de datos de entrenamiento 
pipe.fit(X_train, y_train)
# Se crea la variable de predicción de entrenamiento en Sklearn 
y_train_hatSk = pipe.predict(X_train)
y_train_hatSk

print("Partición de entrenamento")
print(y_train.describe())

print("Partición de test")
print(y_test.describe())

# Se estandarizan las columnas numéricas se realiza el preprocesado
numeric_cols = X_train.select_dtypes(include=['float64', 'int']).columns.to_list()
cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.to_list()

preprocessor = ColumnTransformer(
                   [('scale', StandardScaler(), numeric_cols),
                    ('onehot', OneHotEncoder(handle_unknown='ignore'), cat_cols)],
                remainder='passthrough')

X_train_prep = preprocessor.fit_transform(X_train)
X_test_prep  = preprocessor.transform(X_test)

# Se realiza el Modelo en RL en Sklearn: Se hace uso de la actualización
# actualización de Sklearn para los modelos de predicción
pipe = Pipeline([('scaler', StandardScaler()), ('LR', LinearRegression())])

# Se entrena el modelo para el conjunto de datos de entrenamiento 
pipe.fit(X_train, y_train)

# Se crea la variable de predicción de entrenamiento en Sklearn 
y_train_hatSk = pipe.predict(X_train)

# Validación cruzada repetida con múltiples métricas

cv = RepeatedKFold(n_splits=3, n_repeats=5, random_state=123)
cv_scores = cross_validate(
                estimator = pipe,
                X         = X_train,
                y         = y_train,
                scoring   = ('r2', 'neg_root_mean_squared_error'),
                cv        = cv,
                return_train_score = True
            )

# Se convierte el diccionario a dataframe para facilitar la visualización
cv_scores = pd.DataFrame(cv_scores)
print("Se muestran dos metricas para la evaluacion del modelo")
cv_scores

y_train_hatSk

# Se toman los datos para test de las variables independientes para 
# que el modelo sea contrastado con los valores reales
predicciones_test = pipe.predict(X_test)
# Se crea y muestra un data frame con las estimaciones y los valores reales
df_predicciones_test = pd.DataFrame({'Porcentaje de pobreza' : y_test, 'prediccion' : predicciones_test})
df_predicciones_test

# Se toman los datos para test de las variables independientes para 
# que el modelo sea contrastado con los valores reales
predicciones_train = pipe.predict(X_train)
# Se crea y muestra un data frame con las estimaciones y los valores reales
df_predicciones_train = pd.DataFrame({'Porcentaje de pobreza' : y_train, 'prediccion' : predicciones_train})
df_predicciones_train

"""Adicional a las metricas mostradas anteriormente y en relación a las mismas; se manajará la metrica de error cuadratico medio para lograr la evaluación del modelo planteado. El cual es la division por el numero de entradas de la sumatoria al cuadrado de la diferencia entre el valor real y la estimacion. En esta metrica el valor de 0 es un modelo perfecto y cuanto mas alejado de 0 se encuentre este valor, menos preciso es el modelo."""

# Se calcula y muestra el valor de la metrica para su posterior analisis
rmse = mean_squared_error(
        y_true = y_test,
        y_pred = predicciones_test,
        squared = False
       )
print("El valor calculado en el error cuadratico medio es:",rmse)
print("Lo que nos indica que nuestro modelo es: poco preciso, esto se puede") 
print("deber a que para un modelo predictivo suelen ser necesarias cantidades de datos muy grandes")
print("tambien se induce que la data suministrada se encuentra poco uniforme, ")
print("es decir sus valores se alejan demasiado de la media de los mismos")

"""# Webgrafia
https://www.delftstack.com/es/howto/python/standard-deviation-of-a-list-in-python/
https://aprendeia.com/evaluando-el-error-en-los-modelos-de-regresion/
https://www.cienciadedatos.net/documentos/py06_machine_learning_python_scikitlearn.html

# **Presentación de graficos de C++ y sus respectivas comparaciones**

Se muestran las graficas comparativas entre los dos sets de datos predichos para entrenamiento por los modelos.
Azul para el extraido de C++ y rojo para el de python.
"""

# Se carga a un objeto dataFrame la informacion de las predicciones de C++
prediccionesC_train = pd.read_csv("/content/y_train_hatCPP.csv")


# Se pinta la distribución de las variables. 
# Se pintaran dos variables (dependiente -- independiente)
fig, axes = plt.subplots(1,2, figsize=(12,5))
plt.suptitle('Distribución predicciones')
sns.histplot(prediccionesC_train, ax=axes[0],kde=True)
sns.histplot(predicciones_train, ax=axes[1],kde=True, color='r')
plt.show()

"""Se muestran las graficas comparativas entre los dos sets de datos predichos para prueba por los modelos.
Azul para el extraido de C++ y rojo para el de python.
"""

# Se carga a un objeto dataFrame la informacion de las predicciones de C++
prediccionesC_test = pd.read_csv("/content/y_test_hatCPP.csv")


# Se pinta la distribución de las variables. 
# Se pintaran dos variables (dependiente -- independiente)
fig, axes = plt.subplots(1,2, figsize=(12,5))
plt.suptitle('Distribución de Predicciones')
sns.histplot(prediccionesC_test, ax=axes[0],kde=True)
sns.histplot(predicciones_test, ax=axes[1],kde=True, color='r')
plt.show()